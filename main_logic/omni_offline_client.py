# -- coding: utf-8 --

import asyncio
from typing import Optional, Callable, Dict, Any, Awaitable
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from openai import APIConnectionError, InternalServerError, RateLimitError
from config import get_extra_body
from utils.frontend_utils import calculate_text_similarity, count_words_and_chars
from utils.logger_config import get_module_logger

# Setup logger for this module
logger = get_module_logger(__name__, "Main")

class OmniOfflineClient:
    """
    A client for text-based chat that mimics the interface of OmniRealtimeClient.
    
    This class provides a compatible interface with OmniRealtimeClient but uses
    langchain's ChatOpenAI with OpenAI-compatible API instead of realtime WebSocket,
    suitable for text-only conversations.
    
    Attributes:
        base_url (str):
            The base URL for the OpenAI-compatible API (e.g., OPENROUTER_URL).
        api_key (str):
            The API key for authentication.
        model (str):
            Model to use for chat.
        vision_model (str):
            Model to use for vision tasks.
        vision_base_url (str):
            Optional separate base URL for vision model API.
        vision_api_key (str):
            Optional separate API key for vision model.
        llm (ChatOpenAI):
            Langchain ChatOpenAI client for streaming text generation.
        on_text_delta (Callable[[str, bool], Awaitable[None]]):
            Callback for text delta events.
        on_input_transcript (Callable[[str], Awaitable[None]]):
            Callback for input transcript events (user messages).
        on_output_transcript (Callable[[str, bool], Awaitable[None]]):
            Callback for output transcript events (assistant messages).
        on_connection_error (Callable[[str], Awaitable[None]]):
            Callback for connection errors.
        on_response_done (Callable[[], Awaitable[None]]):
            Callback when a response is complete.
    """
    def __init__(
        self,
        base_url: str,
        api_key: str,
        model: str = "",
        vision_model: str = "",
        vision_base_url: str = "",  # ç‹¬ç«‹çš„è§†è§‰æ¨¡å‹ API URL
        vision_api_key: str = "",   # ç‹¬ç«‹çš„è§†è§‰æ¨¡å‹ API Key
        voice: str = "",  # Unused for text mode but kept for compatibility
        turn_detection_mode = None,  # Unused for text mode
        on_text_delta: Optional[Callable[[str, bool], Awaitable[None]]] = None,
        on_audio_delta: Optional[Callable[[bytes], Awaitable[None]]] = None,  # Unused
        on_interrupt: Optional[Callable[[], Awaitable[None]]] = None,  # Unused
        on_input_transcript: Optional[Callable[[str], Awaitable[None]]] = None,
        on_output_transcript: Optional[Callable[[str, bool], Awaitable[None]]] = None,
        on_connection_error: Optional[Callable[[str], Awaitable[None]]] = None,
        on_response_done: Optional[Callable[[], Awaitable[None]]] = None,
        on_repetition_detected: Optional[Callable[[], Awaitable[None]]] = None,
        on_response_discarded: Optional[Callable[[str, int, int, bool, Optional[str]], Awaitable[None]]] = None,
        extra_event_handlers: Optional[Dict[str, Callable[[Dict[str, Any]], Awaitable[None]]]] = None,
        max_response_length: Optional[int] = None
    ):
        # Use base_url directly without conversion
        self.base_url = base_url
        self.api_key = api_key if api_key and api_key != '' else None
        self.model = model
        self.vision_model = vision_model  # Store vision model for temporary switching
        # è§†è§‰æ¨¡å‹ç‹¬ç«‹é…ç½®ï¼ˆå¦‚æœæœªæŒ‡å®šåˆ™å›é€€åˆ°ä¸»é…ç½®ï¼‰
        self.vision_base_url = vision_base_url if vision_base_url else base_url
        self.vision_api_key = vision_api_key if vision_api_key else api_key
        self.on_text_delta = on_text_delta
        self.on_input_transcript = on_input_transcript
        self.on_output_transcript = on_output_transcript
        self.handle_connection_error = on_connection_error
        self.on_response_done = on_response_done
        self.on_repetition_detected = on_repetition_detected
        self.on_response_discarded = on_response_discarded
        
        # Initialize langchain ChatOpenAI client
        self.llm = ChatOpenAI(
            model=self.model,
            base_url=self.base_url,
            api_key=self.api_key,
            temperature=1.0,
            streaming=True,
            max_retries=0,  # ç¦ç”¨ openai client å†…ç½®é‡è¯•ï¼Œç”±å¤–å±‚ retry loop å¤„ç†ï¼ˆå†…ç½®é‡è¯•ä¼šç ´åæµå¼ generatorï¼‰
            extra_body=get_extra_body(self.model) or None
        )
        
        # State management
        self._is_responding = False
        self._conversation_history = []
        self._instructions = ""
        self._stream_task = None
        self._pending_images = []  # Store pending images to send with next text
        
        # é‡å¤åº¦æ£€æµ‹
        self._recent_responses = []  # å­˜å‚¨æœ€è¿‘3è½®åŠ©æ‰‹å›å¤
        self._repetition_threshold = 0.8  # ç›¸ä¼¼åº¦é˜ˆå€¼
        self._max_recent_responses = 3  # æœ€å¤šå­˜å‚¨çš„å›å¤æ•°
        
        # ========== æ™®é€šå¯¹è¯å®ˆå«é…ç½® ==========
        self.enable_response_guard = True     # æ˜¯å¦å¯ç”¨è´¨é‡å®ˆå«
        self.max_response_length = max_response_length if isinstance(max_response_length, int) and max_response_length > 0 else 400
        self.max_response_rerolls = 2         # æœ€å¤šå…è®¸çš„è‡ªåŠ¨é‡è¯•æ¬¡æ•°
        
        # è´¨é‡å®ˆå«å›è°ƒï¼šç”± core.py è®¾ç½®ï¼Œç”¨äºé€šçŸ¥å‰ç«¯æ¸…ç†æ°”æ³¡
        
    async def connect(self, instructions: str, native_audio=False) -> None:
        """Initialize the client with system instructions."""
        self._instructions = instructions
        # Add system message to conversation history using langchain format
        self._conversation_history = [
            SystemMessage(content=instructions)
        ]
        logger.info("OmniOfflineClient initialized with instructions")
    
    async def send_event(self, event) -> None:
        """Compatibility method - not used in text mode"""
        pass
    
    async def update_session(self, config: Dict[str, Any]) -> None:
        """Compatibility method - update instructions if provided"""
        if "instructions" in config:
            self._instructions = config["instructions"]
            # Update system message using langchain format
            if self._conversation_history and isinstance(self._conversation_history[0], SystemMessage):
                self._conversation_history[0] = SystemMessage(content=self._instructions)
    
    def switch_model(self, new_model: str, use_vision_config: bool = False) -> None:
        """
        Temporarily switch to a different model (e.g., vision model).
        This allows dynamic model switching for vision tasks.
        
        Args:
            new_model: The model to switch to
            use_vision_config: If True, use vision_base_url and vision_api_key
        """
        if new_model and new_model != self.model:
            logger.info(f"Switching model from {self.model} to {new_model}")
            self.model = new_model
            
            # é€‰æ‹©ä½¿ç”¨çš„ API é…ç½®
            if use_vision_config:
                base_url = self.vision_base_url
                api_key = self.vision_api_key if self.vision_api_key and self.vision_api_key != '' else None
            else:
                base_url = self.base_url
                api_key = self.api_key
            
            # Recreate LLM instance with new model and config
            self.llm = ChatOpenAI(
                model=self.model,
                base_url=base_url,
                api_key=api_key,
                temperature=1.0,
                streaming=True,
                max_retries=0,  # ç¦ç”¨å†…ç½®é‡è¯•
                extra_body=get_extra_body(self.model) or None
            )
    
    async def _check_repetition(self, response: str) -> bool:
        """
        æ£€æŸ¥å›å¤æ˜¯å¦ä¸è¿‘æœŸå›å¤é«˜åº¦é‡å¤ã€‚
        å¦‚æœè¿ç»­3è½®éƒ½é«˜åº¦é‡å¤ï¼Œè¿”å› True å¹¶è§¦å‘å›è°ƒã€‚
        """
        
        # ä¸æœ€è¿‘çš„å›å¤æ¯”è¾ƒç›¸ä¼¼åº¦
        high_similarity_count = 0
        for recent in self._recent_responses:
            similarity = calculate_text_similarity(response, recent)
            if similarity >= self._repetition_threshold:
                high_similarity_count += 1
        
        # æ·»åŠ åˆ°æœ€è¿‘å›å¤åˆ—è¡¨
        self._recent_responses.append(response)
        if len(self._recent_responses) > self._max_recent_responses:
            self._recent_responses.pop(0)
        
        # å¦‚æœä¸æœ€è¿‘2è½®éƒ½é«˜åº¦é‡å¤ï¼ˆå³ç¬¬3è½®é‡å¤ï¼‰ï¼Œè§¦å‘æ£€æµ‹
        if high_similarity_count >= 2:
            logger.warning(f"OmniOfflineClient: æ£€æµ‹åˆ°è¿ç»­{high_similarity_count + 1}è½®é«˜é‡å¤åº¦å¯¹è¯")
            
            # æ¸…ç©ºå¯¹è¯å†å²ï¼ˆä¿ç•™ç³»ç»ŸæŒ‡ä»¤ï¼‰
            if self._conversation_history and isinstance(self._conversation_history[0], SystemMessage):
                self._conversation_history = [self._conversation_history[0]]
            else:
                self._conversation_history = []
            
            # æ¸…ç©ºé‡å¤æ£€æµ‹ç¼“å­˜
            self._recent_responses.clear()
            
            # è§¦å‘å›è°ƒ
            if self.on_repetition_detected:
                await self.on_repetition_detected()
            
            return True
        
        return False

    async def _notify_response_discarded(self, reason: str, attempt: int, max_attempts: int, will_retry: bool,
                                         message: Optional[str] = None) -> None:
        """
        é€šçŸ¥ä¸Šå±‚å½“å‰å›å¤è¢«ä¸¢å¼ƒï¼Œç”¨äºæ¸…ç©ºå‰ç«¯æ°”æ³¡/æç¤ºç”¨æˆ·
        """
        if self.on_response_discarded:
            try:
                await self.on_response_discarded(reason, attempt, max_attempts, will_retry, message)
            except Exception as e:
                logger.warning(f"é€šçŸ¥ response_discarded å¤±è´¥: {e}")

    async def stream_text(self, text: str) -> None:
        """
        Send a text message to the API and stream the response.
        If there are pending images, temporarily switch to vision model for this turn.
        Uses langchain ChatOpenAI for streaming.
        """
        if not text or not text.strip():
            # If only images without text, use a default prompt
            if self._pending_images:
                text = "è¯·åˆ†æè¿™äº›å›¾ç‰‡ã€‚"
            else:
                return
        
        # Check if we need to switch to vision model
        has_images = len(self._pending_images) > 0
        
        # Prepare user message content
        if has_images:
            # Switch to vision model permanently for this session
            # (cannot switch back because image data remains in conversation history)
            if self.vision_model and self.vision_model != self.model:
                logger.info(f"ğŸ–¼ï¸ Temporarily switching to vision model: {self.vision_model} (from {self.model})")
                self.switch_model(self.vision_model, use_vision_config=True)
            
            # Multi-modal message: images + text
            content = []
            
            # Add images first
            for img_b64 in self._pending_images:
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{img_b64}"
                    }
                })
            
            # Add text
            content.append({
                "type": "text",
                "text": text.strip()
            })
            
            user_message = HumanMessage(content=content)
            logger.info(f"Sending multi-modal message with {len(self._pending_images)} images")
            
            # Clear pending images after using them
            self._pending_images.clear()
        else:
            # Text-only message
            user_message = HumanMessage(content=text.strip())
        
        self._conversation_history.append(user_message)
        
        # Callback for user input
        if self.on_input_transcript:
            await self.on_input_transcript(text.strip())
        
        # Retryç­–ç•¥ï¼šé‡è¯•2æ¬¡ï¼Œé—´éš”1ç§’ã€2ç§’
        max_retries = 3
        retry_delays = [1, 2]
        assistant_message = ""
        
        try:
            self._is_responding = True
            reroll_count = 0
            
            # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿å¯¹è¯å†å²ä¸­è‡³å°‘æœ‰ç”¨æˆ·æ¶ˆæ¯
            has_user_message = any(isinstance(msg, HumanMessage) for msg in self._conversation_history)
            if not has_user_message:
                error_msg = "å¯¹è¯å†å²ä¸­æ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œæ— æ³•ç”Ÿæˆå›å¤"
                logger.error(f"OmniOfflineClient: {error_msg}")
                if self.handle_connection_error:
                    await self.handle_connection_error(error_msg)
                return
            
            guard_exhausted = False
            for attempt in range(max_retries):
                try:
                    assistant_message = ""
                    guard_attempt = 0
                    while guard_attempt <= self.max_response_rerolls:
                        self._is_responding = True
                        assistant_message = ""
                        is_first_chunk = True
                        pipe_count = 0  # å›´æ ï¼šè¿½è¸ª | å­—ç¬¦çš„å‡ºç°æ¬¡æ•°
                        fence_triggered = False  # å›´æ æ˜¯å¦å·²è§¦å‘
                        guard_triggered = False
                        discard_reason = None
                        
                        async for chunk in self.llm.astream(self._conversation_history):
                            if not self._is_responding:
                                break
                            
                            if fence_triggered:
                                break
                            
                            content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                            
                            if content and content.strip():
                                truncated_content = content
                                for idx, char in enumerate(content):
                                    if char == '|':
                                        pipe_count += 1
                                        if pipe_count >= 2:
                                            truncated_content = content[:idx]
                                            fence_triggered = True
                                            logger.info("OmniOfflineClient: å›´æ è§¦å‘ - æ£€æµ‹åˆ°ç¬¬äºŒä¸ª | å­—ç¬¦ï¼Œæˆªæ–­è¾“å‡º")
                                            break
                                
                                if truncated_content and truncated_content.strip():
                                    assistant_message += truncated_content
                                    if self.on_text_delta:
                                        await self.on_text_delta(truncated_content, is_first_chunk)
                                    is_first_chunk = False
                                    
                                    if self.enable_response_guard:
                                        current_length = count_words_and_chars(assistant_message)
                                        if current_length > self.max_response_length:
                                            guard_triggered = True
                                            discard_reason = f"length>{self.max_response_length}"
                                            logger.info(f"OmniOfflineClient: æ£€æµ‹åˆ°é•¿å›å¤ ({current_length}å­—)ï¼Œå‡†å¤‡é‡è¯•")
                                            self._is_responding = False
                                            break
                            elif content and not content.strip():
                                logger.debug(f"OmniOfflineClient: è¿‡æ»¤ç©ºç™½å†…å®¹ - content_repr: {repr(content)[:100]}")
                        
                        if guard_triggered:
                            guard_attempt += 1
                            reroll_count += 1
                            will_retry = guard_attempt <= self.max_response_rerolls
                            # åŒºåˆ†åŸå› ï¼šè¶…é•¿ç”¨æ˜ç¡®æç¤ºï¼Œå…¶å®ƒå®ˆå«åŸå› ç”¨é€šç”¨æç¤º
                            if discard_reason and "length>" in discard_reason:
                                final_message = "å›å¤è¿‡é•¿ï¼Œå·²æ”¾å¼ƒè¾“å‡ºï¼ˆå¯åœ¨é…ç½®ä¸­è°ƒå¤§ TEXT_GUARD_MAX_LENGTHï¼‰"
                            else:
                                final_message = "AIå›å¤ä¸ç¬¦åˆè¦æ±‚ï¼Œå·²æ”¾å¼ƒè¾“å‡º"
                            failure_message = None if will_retry else final_message
                            await self._notify_response_discarded(
                                discard_reason or "guard",
                                guard_attempt,
                                self.max_response_rerolls,
                                will_retry,
                                failure_message
                            )
                            
                            if will_retry:
                                logger.info(f"OmniOfflineClient: å“åº”è¢«ä¸¢å¼ƒï¼ˆ{discard_reason}ï¼‰ï¼Œç¬¬ {guard_attempt}/{self.max_response_rerolls} æ¬¡é‡è¯•")
                                continue
                            
                            logger.warning("OmniOfflineClient: guard é‡è¯•è€—å°½ï¼Œæ”¾å¼ƒè¾“å‡º")
                            if self.handle_connection_error:
                                await self.handle_connection_error(final_message)
                            assistant_message = ""
                            guard_exhausted = True
                            break
                        
                        if assistant_message:
                            self._conversation_history.append(AIMessage(content=assistant_message))
                            await self._check_repetition(assistant_message)
                        break
                    
                    if guard_exhausted:
                        break
                    
                    if assistant_message:
                        break
                            
                except (APIConnectionError, InternalServerError, RateLimitError) as e:
                    logger.info(f"â„¹ï¸ æ•è·åˆ° {type(e).__name__} é”™è¯¯")
                    if attempt < max_retries - 1:
                        wait_time = retry_delays[attempt]
                        logger.warning(f"OmniOfflineClient: LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries})ï¼Œ{wait_time}ç§’åé‡è¯•: {e}")
                        # é€šçŸ¥å‰ç«¯æ­£åœ¨é‡è¯•
                        if self.handle_connection_error:
                            await self.handle_connection_error(f"è¿æ¥é—®é¢˜ï¼Œæ­£åœ¨é‡è¯•...ï¼ˆç¬¬{attempt + 1}æ¬¡ï¼‰")
                        await asyncio.sleep(wait_time)
                        continue  # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
                    else:
                        error_msg = f"LLMè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {str(e)}"
                        logger.error(error_msg)
                        if self.handle_connection_error:
                            await self.handle_connection_error(error_msg)
                        break
                except Exception as e:
                    error_msg = f"Error in text streaming: {str(e)}"
                    logger.error(error_msg)
                    if self.handle_connection_error:
                        await self.handle_connection_error(error_msg)
                    break  # éé‡è¯•ç±»é”™è¯¯ç›´æ¥é€€å‡º
        finally:
            self._is_responding = False
            
            # ç©ºå›å¤å…œåº•ï¼šå¦‚æœæ‰€æœ‰é‡è¯•éƒ½æœªäº§ç”Ÿæ–‡æœ¬ï¼Œå‘å‰ç«¯å‘é€é”™è¯¯æç¤º
            if not assistant_message and not guard_exhausted:
                logger.warning("OmniOfflineClient: æ‰€æœ‰é‡è¯•å‡æœªäº§ç”Ÿæ–‡æœ¬å›å¤")
                if self.on_text_delta:
                    fallback_msg = "ï¼ˆæœåŠ¡æš‚æ—¶ä¸ç¨³å®šï¼Œè¯·å†è¯•ä¸€æ¬¡ï¼‰"
                    await self.on_text_delta(fallback_msg, True)
            
            # Call response done callback
            if self.on_response_done:
                await self.on_response_done()
    
    async def stream_audio(self, audio_chunk: bytes) -> None:
        """Compatibility method - not used in text mode"""
        pass
    
    async def stream_image(self, image_b64: str) -> None:
        """
        Add an image to pending images queue.
        Images will be sent together with the next text message.
        """
        if not image_b64:
            return
        
        # Store base64 image
        self._pending_images.append(image_b64)
        logger.info(f"Added image to pending queue (total: {len(self._pending_images)})")
    
    def has_pending_images(self) -> bool:
        """Check if there are pending images waiting to be sent."""
        return len(self._pending_images) > 0
    
    async def create_response(self, instructions: str, skipped: bool = False) -> None:
        """
        Process a system message or instruction.
        For compatibility with OmniRealtimeClient interface.
        """
        # Extract actual instruction if it starts with "SYSTEM_MESSAGE | "
        if instructions.startswith("SYSTEM_MESSAGE | "):
            instructions = instructions[17:]  # Remove prefix
        
        # Add as system message using langchain format
        if instructions.strip():
            self._conversation_history.append(SystemMessage(content=instructions))
    
    async def stream_proactive(self, instruction: str) -> bool:
        """Generate and stream a proactive AI response driven by a system instruction.

        The *instruction* is expected to be pre-formatted by the caller using the
        ========...======== convention and is injected as a temporary HumanMessage.
        It is **not** persisted to _conversation_history.  Only the AI's
        natural-language response (AIMessage) is kept in history.

        Calls on_response_done() when finished (same as stream_text).
        Returns True if any text was generated, False if aborted or empty.
        """
        if not instruction or not instruction.strip():
            return False

        # ä¸´æ—¶æ³¨å…¥ï¼šinstruction å·²ç”±è°ƒç”¨æ–¹ç”¨ ======== æ ¼å¼å°è£…ï¼Œä½œä¸º HumanMessage å‘é€ï¼Œ
        # ä¸æŒä¹…åŒ–åˆ° _conversation_historyï¼Œé¿å…æ±¡æŸ“é•¿æœŸä¸Šä¸‹æ–‡ã€‚
        messages_to_send = (
            self._conversation_history
            + [HumanMessage(content=instruction)]
        )

        assistant_message = ""
        is_first_chunk = True

        try:
            self._is_responding = True
            async for chunk in self.llm.astream(messages_to_send):
                if not self._is_responding:
                    break
                content = chunk.content if hasattr(chunk, "content") else str(chunk)
                if content and content.strip():
                    assistant_message += content
                    if self.on_text_delta:
                        await self.on_text_delta(content, is_first_chunk)
                    is_first_chunk = False
        except Exception as e:
            error_msg = f"OmniOfflineClient.stream_proactive error: {e}"
            logger.error(error_msg)
            if self.handle_connection_error:
                await self.handle_connection_error(error_msg)
            assistant_message = ""  # é˜²æ­¢æ®‹ç¼ºå†…å®¹è¢« finally å†™å…¥å†å²
            return False
        finally:
            self._is_responding = False
            if assistant_message:
                self._conversation_history.append(AIMessage(content=assistant_message))
            if self.on_response_done:
                await self.on_response_done()

        return bool(assistant_message)

    async def cancel_response(self) -> None:
        """Cancel the current response if possible"""
        self._is_responding = False
        # Stop processing new chunks by setting flag
    
    async def handle_interruption(self):
        """Handle user interruption - cancel current response"""
        if not self._is_responding:
            return
        
        logger.info("Handling text mode interruption")
        await self.cancel_response()
    
    async def handle_messages(self) -> None:
        """
        Compatibility method for OmniRealtimeClient interface.
        In text mode, this is a no-op as we don't have a persistent connection.
        """
        # Keep this task alive to match the interface
        try:
            while True:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            logger.info("Text mode message handler cancelled")
    
    async def close(self) -> None:
        """Close the client and cleanup resources."""
        self._is_responding = False
        self._conversation_history = []
        self._pending_images.clear()
        logger.info("OmniOfflineClient closed")
