# 3サーバー設計

## メインサーバー（`main_server.py`、ポート48911）

メインサーバーは、すべてのインタラクションのユーザー向けエントリーポイントとして機能するFastAPIアプリケーションです。

### 起動シーケンス

1. **設定の読み込み** — `config_manager` の読み込み、キャラクターデータの初期化
2. **セッション作成** — 定義された各キャラクターに対して `LLMSessionManager` を作成
3. **静的ファイルのマウント** — `/static`、`/user_live2d`、`/user_vrm`、`/workshop` をマウント
4. **ルーターの登録** — 10個のAPIルーターをすべて登録
5. **イベントハンドラー** — Steamworksの初期化、ZeroMQブリッジの開始、音声モジュールのプリロード、言語検出
6. **Uvicornの起動** — `127.0.0.1:48911` にバインド

### 処理内容

- すべてのREST APIエンドポイント（10ルーター）
- リアルタイムチャット用WebSocket接続（`/ws/{lanlan_name}`）
- TTS合成（スレッドワーカー）
- 音声リサンプリング（24kHz → 48kHz、soxr経由）
- 静的ファイル配信（モデル、CSS、JS、ロケール）
- HTMLページレンダリング（Jinja2テンプレート）

## メモリサーバー（`memory_server.py`、ポート48912）

メモリサーバーは永続的な会話履歴とセマンティック検索を管理します。

### ストレージレイヤー

| レイヤー | 用途 | バックエンド |
|---------|------|-------------|
| 最近のメモリ | キャラクターごとの直近Nメッセージ | JSONファイル（`recent_*.json`） |
| 時間インデックス付きオリジナル | 完全な会話履歴 | SQLiteテーブル |
| 時間インデックス付き圧縮版 | 要約された履歴 | SQLiteテーブル |
| セマンティックメモリ | Embeddingベースの検索 | ベクトルストア |

### 主要な操作

- **保存**: タイムスタンプ付きで新しい会話ターンを保存
- **クエリ**: LLMプロンプト用の最近のコンテキストを取得
- **検索**: すべての履歴に対するセマンティック類似性検索
- **圧縮**: 古い会話を定期的に要約し、コンテキストウィンドウのスペースを節約
- **レビュー**: ユーザーが保存されたメモリを閲覧・修正可能に

## エージェントサーバー（`agent_server.py`、ポート48915）

エージェントサーバーは、会話コンテキストによってトリガーされるバックグラウンドタスクの実行を処理します。

### ZeroMQアドレッシング

| ソケット | アドレス | 方向 | 用途 |
|---------|---------|------|------|
| PUB/SUB | `tcp://127.0.0.1:48961` | Main → Agent | セッションイベント |
| PUSH/PULL | `tcp://127.0.0.1:48962` | Agent → Main | タスク結果 |
| PUSH/PULL | `tcp://127.0.0.1:48963` | Main → Agent | 分析リクエスト |

### タスク実行パイプライン

1. メインサーバーがZeroMQ経由でタスクをパブリッシュ
2. エージェントサーバーが受信し、タスクプランを作成（`planner.py`）
3. アダプターを通じてアクションを実行：
   - **MCP Client** — Model Context Protocolツール呼び出し
   - **Computer Use** — スクリーンショット分析、マウス/キーボード操作
   - **Browser Use** — Webブラウジング自動化
4. 結果を分析（`analyzer.py`）し、重複を排除（`deduper.py`）
5. 最終結果をZeroMQ経由でストリーミング返却（`task_result`、`proactive_message`）
